{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing coco annotation file for training\n",
    "import pycocotools.coco as coco\n",
    "import os\n",
    "\n",
    "training_image_dir = '/Users/am/Desktop/HKU/InnoWing/project/CNN/data/training-data/img'\n",
    "training_annotation_file = '/Users/am/Desktop/HKU/InnoWing/project/CNN/data/training-data/annotations.json'\n",
    "\n",
    "coco = coco.COCO(training_annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, image_dir, coco_annotations, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco_annotations = coco_annotations\n",
    "        self.img_ids = list(self.coco_annotations.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco_annotations.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.0    # Normalize image to [0, 1]\n",
    "\n",
    "        # Load annotations\n",
    "        ann_ids = self.coco_annotations.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco_annotations.loadAnns(ann_ids)\n",
    "\n",
    "        # Load bounding boxes\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x+w, y+h])\n",
    "            labels.append(1)\n",
    "\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, bboxes, labels\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable numbers of boxes\"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    bboxes = [torch.as_tensor(b, dtype=torch.float32) for b in bboxes]\n",
    "    labels = [torch.as_tensor(l, dtype=torch.int64) for l in labels]\n",
    "    return images, bboxes, labels\n",
    "    \n",
    "training_dataset = TrainingDataset(training_image_dir, coco, transform=transforms.ToTensor())\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_SIZE = [3, 4, 5]\n",
    "ANCHOR_RATIO = [1.0]\n",
    "\n",
    "# Create model class\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # delete max pooling\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 16, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        self.RPN_conv = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        self.RPN_cls_score = nn.Conv2d(32, 1 * len(ANCHOR_SIZE) * len(ANCHOR_RATIO), 1, stride=1, padding=0)    # Binary classification (blob or not)\n",
    "        self.RPN_bbox_pred = nn.Conv2d(32, 4 * len(ANCHOR_SIZE) * len(ANCHOR_RATIO), 1, stride=1, padding=0)    # Bounding box regression for 4 coordinates for each proposed region\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 32)\n",
    "\n",
    "        self.cls_score = nn.Linear(32, 1)  # Binary classification (object or not)\n",
    "        self.bbox_pred = nn.Linear(32, 4)  # Bounding box regression\n",
    "\n",
    "        # Initialize RPN weights\n",
    "        # nn.init.normal_(self.RPN_cls_score.weight, mean=0, std=0.01)  # Small weights for stability\n",
    "        # nn.init.zeros_(self.RPN_cls_score.bias)\n",
    "        # nn.init.normal_(self.RPN_bbox_pred.weight, mean=0, std=0.001)  # Smaller std for regression\n",
    "        # nn.init.zeros_(self.RPN_bbox_pred.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.backbone(x)  # [batch_size, channels, height, width]\n",
    "        \n",
    "        rpn_conv = torch.relu(self.RPN_conv(feature_map))\n",
    "        rpn_cls_score = self.RPN_cls_score(rpn_conv)\n",
    "        rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv)\n",
    "        proposals = self.generate_proposals(rpn_cls_score, rpn_bbox_pred)\n",
    "\n",
    "        proposal_list = []\n",
    "        for i, boxes in enumerate(proposals):\n",
    "            if len(boxes) > 0:\n",
    "                boxes = boxes.view(-1, 4)\n",
    "                batch_indices = torch.full((len(boxes), 1), i, device=boxes.device)\n",
    "                proposal_list.append(torch.cat([batch_indices, boxes], dim=1))  # Add batch index to the front of each proposal\n",
    "\n",
    "        if not proposal_list:\n",
    "            return torch.zeros(1, 2), torch.zeros(1, 4), proposals\n",
    "\n",
    "        # Combine all proposals across batch -> [total_proposals, 5]\n",
    "        all_proposals = torch.cat(proposal_list, dim=0)\n",
    "\n",
    "        # ROI Pool -> [total_proposals, C, 7, 7]\n",
    "        roi_pooled = ops.roi_pool(feature_map, all_proposals, output_size=(7, 7))  \n",
    "\n",
    "        # Flatten -> [total_proposals, 32*7*7]\n",
    "        roi_flatten = roi_pooled.flatten(1)  \n",
    "\n",
    "        # FC layers (bounding head detection)\n",
    "        bbox_head = torch.relu(self.fc1(roi_flatten))  # [total_proposals, 512]\n",
    "        cls_score = self.cls_score(bbox_head).sigmoid()          # [total_proposals, 2]\n",
    "        bbox_pred_delta = self.bbox_pred(bbox_head)         # [total_proposals, 4]\n",
    "        bbox_pred = self.decode_bboxes(all_proposals[:, 1:], bbox_pred_delta)\n",
    "        batch_indices = all_proposals[:, 0]\n",
    "\n",
    "        print(\"Final cls_score:\", cls_score)\n",
    "        print(\"Final bbox_pred delta:\", bbox_pred_delta)\n",
    "\n",
    "        return cls_score, bbox_pred, batch_indices\n",
    "    \n",
    "    def generate_proposals(self, rpn_cls_score, rpn_bbox_pred):\n",
    "        \"\"\"\n",
    "        Generate proposals from RPN scores and bounding box predictions\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = rpn_cls_score.size(0)\n",
    "        all_proposals = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Get scores and deltas for this image\n",
    "            img_scores = rpn_cls_score[i].sigmoid().view(-1) \n",
    "            img_deltas = rpn_bbox_pred[i].view(-1, 4)\n",
    "            \n",
    "            # Generate anchors (same for all images)\n",
    "            anchors = self.generate_anchors(rpn_cls_score.size(2), rpn_cls_score.size(3))\n",
    "\n",
    "            # print(f\"RPN scores: {torch.sigmoid(img_scores).mean():.3f} ± {torch.sigmoid(img_scores).std():.3f}\")\n",
    "            # print(f\"RPN deltas: {img_deltas.mean():.3f} ± {img_deltas.std():.3f}\")\n",
    "            \n",
    "            # Apply deltas to anchors to get proposals\n",
    "            proposals = self.decode_bboxes(anchors, img_deltas)\n",
    "\n",
    "            valid_mask = (\n",
    "                (proposals[:, 0] >= 0) & (proposals[:, 0] <= 32) &  # x1\n",
    "                (proposals[:, 1] >= 0) & (proposals[:, 1] <= 24) &  # y1\n",
    "                (proposals[:, 2] >= 0) & (proposals[:, 2] <= 32) &  # x2\n",
    "                (proposals[:, 3] >= 0) & (proposals[:, 3] <= 24)    # y2\n",
    "            )\n",
    "\n",
    "            proposals = proposals[valid_mask]\n",
    "            img_scores = img_scores[valid_mask]\n",
    "            \n",
    "            # Filter and NMS\n",
    "            keep = img_scores > 0\n",
    "            proposals = proposals[keep]\n",
    "            scores = img_scores[keep]\n",
    "            \n",
    "            if len(proposals) > 0:\n",
    "\n",
    "                keep = ops.nms(proposals, scores, 0.2)\n",
    "                proposals = proposals[keep]\n",
    "\n",
    "            else:\n",
    "                proposals = torch.empty((0, 4), dtype=torch.float32)\n",
    "            \n",
    "            all_proposals.append(proposals)\n",
    "\n",
    "        return all_proposals\n",
    "    \n",
    "    def decode_bboxes(self, anchors, bbox_deltas):\n",
    "        # Anchors: [N,4], bbox_deltas: [N,4]\n",
    "\n",
    "        # Ensure bbox_deltas are within a reasonable range\n",
    "        bbox_deltas = torch.clamp(bbox_deltas, min=-5.0, max=5.0)\n",
    "\n",
    "        widths = anchors[:,2] - anchors[:,0]\n",
    "        heights = anchors[:,3] - anchors[:,1]\n",
    "        ctr_x = anchors[:,0] + 0.5 * widths\n",
    "        ctr_y = anchors[:,1] + 0.5 * heights\n",
    "        \n",
    "        dx = bbox_deltas[:,0] * widths\n",
    "        dy = bbox_deltas[:,1] * heights\n",
    "        dw = torch.sigmoid(bbox_deltas[:,2]) * 2.0  # Constrain to (0, 2)\n",
    "        dh = torch.sigmoid(bbox_deltas[:,3]) * 2.0\n",
    "        \n",
    "        pred_ctr_x = ctr_x + dx\n",
    "        pred_ctr_y = ctr_y + dy\n",
    "        pred_w = dw\n",
    "        pred_h = dh\n",
    "        \n",
    "        pred_boxes = torch.zeros_like(bbox_deltas)\n",
    "        pred_boxes[:,0] = pred_ctr_x - 0.5 * pred_w  # x1\n",
    "        pred_boxes[:,1] = pred_ctr_y - 0.5 * pred_h  # y1\n",
    "        pred_boxes[:,2] = pred_ctr_x + 0.5 * pred_w  # x2\n",
    "        pred_boxes[:,3] = pred_ctr_y + 0.5 * pred_h  # y2\n",
    "        \n",
    "        return pred_boxes\n",
    "    \n",
    "    def generate_anchors(self, height, width):\n",
    "        \"\"\"\n",
    "        Generate anchors for the given feature map size\n",
    "        \"\"\"\n",
    "        anchors = []\n",
    "        stride = (2, 2)\n",
    "        for x in range(0, width):\n",
    "            for y in range(0, height):\n",
    "                for scale in ANCHOR_SIZE:\n",
    "                    for ratio in ANCHOR_RATIO:\n",
    "                        w = scale * math.sqrt(ratio)\n",
    "                        h = scale / math.sqrt(ratio)\n",
    "                        \n",
    "                        # Center at feature map location\n",
    "                        center_x = (x + 0.5) * stride[1]\n",
    "                        center_y = (y + 0.5) * stride[0]\n",
    "\n",
    "                        x1 = max(0, center_x - w/2)\n",
    "                        y1 = max(0, center_y - h/2)\n",
    "                        x2 = max(0, min(32, center_x + w/2))\n",
    "                        y2 = max(0, min(24, center_y + h/2))\n",
    "                        \n",
    "                        anchors.append([\n",
    "                            x1, y1, x2, y2\n",
    "                        ])\n",
    "\n",
    "        anchors = torch.tensor(anchors, dtype=torch.float32)\n",
    "        return anchors\n",
    "    \n",
    "    def compute_loss(self, pred_cls_score, pred_bbox, gt_bboxes, gt_labels):\n",
    "        \"\"\"\n",
    "        Compute loss for the model for each image\n",
    "\n",
    "        Args:\n",
    "        pred_cls_score: [N, 1] tensor of class scores (N = number of proposals)\n",
    "        pred_bbox: [N, 4] tensor of predicted boxes\n",
    "        gt_bboxes: [M, 4] tensor of ground truth boxes (M = number of gt boxes)\n",
    "        gt_labels: [M] tensor of ground truth labels (0 or 1)\n",
    "        \"\"\"\n",
    "\n",
    "        if len(gt_bboxes) == 0 or len(pred_bbox) == 0: \n",
    "            return torch.tensor(0.0, device=pred_cls_score.device, requires_grad=True)\n",
    "        \n",
    "        ious = ops.box_iou(pred_bbox, gt_bboxes) # [N, M]\n",
    "\n",
    "        # For each proposal, find the best matching ground truth box\n",
    "        max_ious, max_indices = ious.max(dim=1) # [N]\n",
    "\n",
    "        # For each proposal, assign 1 if it has a matching ground truth box with IoU > 0.3\n",
    "        assigned_labels = (max_ious > 0.3).float()\n",
    "\n",
    "        # print(f\"Assigned labels: {assigned_labels}\")\n",
    "\n",
    "        # Get the corresponding gt boxes for positive proposals\n",
    "        positive_indices = assigned_labels > 0\n",
    "        matched_gt_boxes = gt_bboxes[max_indices[positive_indices]] # If there are K positive indices, its shape will be [K, 4],\n",
    "\n",
    "        # print(f\"Matched GT boxes: {matched_gt_boxes}\")\n",
    "\n",
    "        # RPN Loss\n",
    "        # rpn_cls_loss = F.binary_cross_entropy_with_logits(\n",
    "        #     pred_cls_score.squeeze(1),  # [N]\n",
    "        #     assigned_labels,            # [N]\n",
    "        #     reduction='mean',\n",
    "        #     pos_weight=torch.tensor([10.0])\n",
    "        # )\n",
    "        rpn_cls_loss = ops.sigmoid_focal_loss(\n",
    "            pred_cls_score.squeeze(1),\n",
    "            assigned_labels,\n",
    "            alpha=0.25,\n",
    "            gamma=3.0,\n",
    "            reduction='mean'\n",
    "        )\n",
    "\n",
    "        # Bounding Box Loss (Smooth L1 Loss)\n",
    "        # if positive_indices.sum() > 0:\n",
    "        #     bbox_loss = F.smooth_l1_loss(\n",
    "        #         pred_bbox[positive_indices], \n",
    "        #         matched_gt_boxes, \n",
    "        #         reduction='mean'\n",
    "        #     )\n",
    "        # else:\n",
    "        #     bbox_loss = torch.tensor(0.0, device=pred_cls_score.device, requires_grad=True)\n",
    "\n",
    "        total_loss = rpn_cls_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize model instance\n",
    "model = FasterRCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "num_epochs = 40\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for images, gt_bboxes_list, gt_labels_list in training_dataloader:\n",
    "        # Forward pass\n",
    "        pred_cls_score, pred_bbox, batch_indices = model(images)\n",
    "        \n",
    "        # Process each sample in batch separately\n",
    "        batch_loss = 0\n",
    "        valid_samples = 0\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            # print(f\"Processing sample {i} in batch {batch_indices[i].item()} epoch {epoch+1}\")\n",
    "            # Get predictions for each image\n",
    "            img_mask = (batch_indices == i)\n",
    "            img_pred_cls = pred_cls_score[img_mask]\n",
    "            img_pred_bbox = pred_bbox[img_mask]\n",
    "            img_gt_boxes = gt_bboxes_list[i]\n",
    "            \n",
    "            # Get ground truth for this sample\n",
    "            img_gt_bboxes = gt_bboxes_list[i]\n",
    "            img_gt_labels = gt_labels_list[i]\n",
    "            \n",
    "            # Skip if no ground truth boxes\n",
    "            if len(img_gt_bboxes) == 0:\n",
    "                continue\n",
    "\n",
    "            loss = model.compute_loss(\n",
    "                img_pred_cls,\n",
    "                img_pred_bbox,\n",
    "                img_gt_bboxes,\n",
    "                img_gt_labels\n",
    "            )\n",
    "            if not torch.isnan(loss):\n",
    "                valid_samples += 1\n",
    "                batch_loss += loss\n",
    "        \n",
    "        # Only backprop if we had valid samples\n",
    "        if batch_loss > 0:\n",
    "            # Average loss over batch\n",
    "            loss = batch_loss / valid_samples\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(training_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_header(tensor, var_name, file_path, flatten=True):\n",
    "    \"\"\"Export tensor to C++ header file\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    data = tensor.detach().numpy()\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(\"// Auto-generated weights from PyTorch\\n\\n\")\n",
    "        \n",
    "        if len(data.shape) == 1:\n",
    "            # Biases (1D array)\n",
    "            f.write(\"const float {}[{}] = {{\\n\".format(var_name, len(data)))\n",
    "            for value in data:\n",
    "                f.write(\"    {:.6f}f,\\n\".format(value))\n",
    "            f.write(\"};\\n\")\n",
    "            \n",
    "        elif len(data.shape) == 2:\n",
    "            # FC weights (2D array)\n",
    "            f.write(\"const float {}[{}][{}] = {{\\n\".format(\n",
    "                var_name, data.shape[0], data.shape[1]))\n",
    "            for i in range(data.shape[0]):\n",
    "                f.write(\"    {\")\n",
    "                for j in range(data.shape[1]):\n",
    "                    f.write(\"{:.6f}f\".format(data[i,j]))\n",
    "                    if j < data.shape[1]-1:\n",
    "                        f.write(\", \")\n",
    "                f.write(\"}\")\n",
    "                if i < data.shape[0]-1:\n",
    "                    f.write(\",\")\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"};\\n\")\n",
    "            \n",
    "        elif len(data.shape) == 4:\n",
    "            # Conv weights (4D array)\n",
    "            f.write(\"const float {}[{}][{}][{}][{}] = {{\\n\".format(\n",
    "                var_name, data.shape[0], data.shape[1], \n",
    "                data.shape[2], data.shape[3]))\n",
    "            for i in range(data.shape[0]):  # out channels\n",
    "                f.write(\"    {\")  # Start of filter block\n",
    "                f.write(\" // Filter %d\\n\" % i)  # Comment\n",
    "                for j in range(data.shape[1]):  # in channels\n",
    "                    f.write(\"        {\")  # Start of input channel block\n",
    "                    for k in range(data.shape[2]):  # kernel dim 1\n",
    "                        for l in range(data.shape[3]):  # kernel dim 2\n",
    "                            f.write(\"{:.6f}f\".format(data[i,j,k,l]))\n",
    "                            if l < data.shape[3]-1 or k < data.shape[2]-1:\n",
    "                                f.write(\", \")\n",
    "                    f.write(\"}\")\n",
    "                    if j < data.shape[1]-1:\n",
    "                        f.write(\",\")\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(\"    }\") \n",
    "                if i < data.shape[0]-1:\n",
    "                    f.write(\",\")\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"};\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model, output_dir=\"exported_weights\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export backbone\n",
    "    for i, layer in enumerate(model.backbone):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            export_to_header(layer.weight, f\"conv{i+1}_weights\", f\"{output_dir}/conv{i+1}_weights.h\")\n",
    "            if layer.bias is not None:\n",
    "                export_to_header(layer.bias, f\"conv{i+1}_biases\", f\"{output_dir}/conv{i+1}_biases.h\")\n",
    "    \n",
    "    # Export RPN\n",
    "    export_to_header(model.RPN_conv.weight, \"rpn_conv_weights\", f\"{output_dir}/rpn_conv_weights.h\")\n",
    "    export_to_header(model.RPN_conv.bias, \"rpn_conv_biases\", f\"{output_dir}/rpn_conv_biases.h\")\n",
    "    export_to_header(model.RPN_cls_score.weight, \"rpn_cls_weights\", f\"{output_dir}/rpn_cls_weights.h\")\n",
    "    export_to_header(model.RPN_cls_score.bias, \"rpn_cls_biases\", f\"{output_dir}/rpn_cls_biases.h\")\n",
    "    export_to_header(model.RPN_bbox_pred.weight, \"rpn_bbox_weights\", f\"{output_dir}/rpn_bbox_weights.h\")\n",
    "    export_to_header(model.RPN_bbox_pred.bias, \"rpn_bbox_biases\", f\"{output_dir}/rpn_bbox_biases.h\")\n",
    "    \n",
    "    # Export detection head\n",
    "    export_to_header(model.fc1.weight, \"fc1_weights\", f\"{output_dir}/fc1_weights.h\")\n",
    "    export_to_header(model.fc1.bias, \"fc1_biases\", f\"{output_dir}/fc1_biases.h\")\n",
    "    export_to_header(model.cls_score.weight, \"cls_score_weights\", f\"{output_dir}/cls_score_weights.h\")\n",
    "    export_to_header(model.cls_score.bias, \"cls_score_biases\", f\"{output_dir}/cls_score_biases.h\")\n",
    "    export_to_header(model.bbox_pred.weight, \"bbox_pred_weights\", f\"{output_dir}/bbox_pred_weights.h\")\n",
    "    export_to_header(model.bbox_pred.bias, \"bbox_pred_biases\", f\"{output_dir}/bbox_pred_biases.h\")\n",
    "    \n",
    "    print(f\"Model exported to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Export all weights and biases\n",
    "export_model(model, \"exported_weights\")\n",
    "\n",
    "print(\"Export successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, val_loader, device=\"cpu\", show_images=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, gt_boxes_list, _) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            pred_scores, pred_boxes, batch_indices = model(images)\n",
    "            \n",
    "            # Convert to CPU for visualization\n",
    "            images_np = images.cpu().numpy().squeeze(1)  # [B,1,H,W] -> [B,H,W]\n",
    "            pred_scores = torch.sigmoid(pred_scores).cpu()\n",
    "            \n",
    "            # Process each image in batch\n",
    "            for i in range(len(images)):\n",
    "                img = images_np[i]\n",
    "                gt_boxes = gt_boxes_list[i].cpu().numpy()\n",
    "                \n",
    "                # Get predictions for this image\n",
    "                img_mask = (batch_indices == i)\n",
    "                img_pred_scores = pred_scores[img_mask].sigmoid()\n",
    "                img_pred_boxes = pred_boxes[img_mask].cpu().numpy()\n",
    "\n",
    "                print(img_pred_boxes)\n",
    "                \n",
    "                # Filter predictions with score > 0.5\n",
    "                keep = img_pred_scores > 0.5\n",
    "                detections = img_pred_boxes[keep.squeeze()]\n",
    "\n",
    "                # Visualize\n",
    "                if show_images:\n",
    "                    plt.figure(figsize=(10,4))\n",
    "                    plt.imshow(img, cmap='gray')\n",
    "                    \n",
    "                    # Draw ground truth (green)\n",
    "                    for box in gt_boxes:\n",
    "                        plt.plot([box[0], box[2], box[2], box[0], box[0]],\n",
    "                                [box[1], box[1], box[3], box[3], box[1]], 'g-', linewidth=2)\n",
    "                    \n",
    "                    # Draw predictions (red)\n",
    "                    print(f\"Detections: {detections}\")\n",
    "                    for box in detections:\n",
    "                        plt.plot([box[0], box[2], box[2], box[0], box[0]],\n",
    "                                [box[1], box[1], box[3], box[3], box[1]], 'r--', linewidth=1.5)\n",
    "                    \n",
    "                    plt.title(f\"Ground Truth (Green) vs Predictions (Red)\\nDetections: {len(detections)}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Print metrics for this image\n",
    "                    if len(detections) > 0 and len(gt_boxes) > 0:\n",
    "                        iou = ops.box_iou(torch.tensor(detections), torch.tensor(gt_boxes))\n",
    "                        print(f\"Max IoU with GT: {iou.max().item():.2f}\")\n",
    "                    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools.coco as coco\n",
    "\n",
    "validation_image_dir = '/Users/am/Desktop/HKU/InnoWing/project/CNN/data/validation-data/img'\n",
    "validation_annotation_file = '/Users/am/Desktop/HKU/InnoWing/project/CNN/data/validation-data/annotations.json'\n",
    "\n",
    "validation_coco = coco.COCO(validation_annotation_file)\n",
    "\n",
    "validation_dataset = TrainingDataset(validation_image_dir, validation_coco, transform=transforms.ToTensor())\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "validate(model, validation_dataloader, device=\"cpu\", show_images=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
